---
output: github_document
bibliography: references.bib  
---

<!-- badges: start -->
[![R-CMD-check](https://github.com/emilio-berti/assembly/workflows/R-CMD-check/badge.svg)](https://github.com/emilio-berti/assembly/actions)
<!-- badges: end -->


<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  fig.width = 4,
  fig.height = 4,
  fig.align='center',
  dpi = 300,
  out.width = "50%"
)
```

# Getting started

## Background

Following food web terminology, I talk here about _resources_ and _consumers_. In the case of plant-pollinator networks, this is analogous as replacing _resource_ with _plant_ and _consumers_ with _pollinator_. In this case, however, some filtering steps may be unnecessary and unwanted. __More about this in a separate section.__ I also talk about metawebs, which are the regional food web networks that emerge considering all interactions that occur in local communities within the region of interest.

The general scope of _assembly_ is to simulate top-down assembly processes. Top-down assembly means that all species are introduced into a local community at once. The opposite, i.e. communities are built by sequential introductions of one species, is called bottom-up assembly. Bottom-up assembly is problematic for community composed of many species, as the number of unique assembly sequences is $S!$, where $S$ is the number of species in the metaweb. For 100 species, for instance, there are $\sim 10^{157}$ unique sequences, making computations (and replications) virtually impossible. Strikingly, @servan2021tractable showed that bottop-up and top-down assembly are equivalent under some specific conditions. 

Far from assuming this is the case in _assembly_, I simply want to highlight that _assembly_ only implements top-down assembly and that none of the procedures in _assembly_ can be considered steps in an ecological sequence. Whenever I talk about _steps_, _moves_, and _sequences_ in _assembly_, I always refer to _procedures steps_, _procedures moves_, and _procedures sequences_. Always keep that in mind and don't be fooled by the terminology; there is no bottom-up assembly in _assembly_.

At this point, if you're like me, you will ask _why not bottom-up assembly?_ The simple answer is: because it is computational unfeasible for large communities. To make it feasible, it is tempting to restrict the possible sequences to a (very) narrow space, only to 1,000,000 sequences ($10^{-152}$ of the total fraction of possible sequences for 100 species; much _much_ smaller than considering only a grain of sand simulating the whole Earth). As this seems quite dangerous, potentially leading to biased conclusions, I completed neglected bottom-up assembly processes.

## General workflow of _assembly_

In general, _assembly_ implements this pipeline:

  1. Draw random species from a metaweb.
  2. Impose resource filtering, i.e. each basal species must be consumed and each consumer must feed on a resource.
  3. Impose limiting similarity filtering, i.e. consumers are replaced by others, depending on a probability distribution proportional to their similarity of interactions.
  
These steps are not necessarily sequential, i.e. limiting similarity can be imposed independently from the resource filtering, but this is programmatically harder. Because ecological communities are likely filtered by resource availability before interaction competition, I wrote _assembly_ in a way that is straightforward to pass the output of the resource filtering procedure to the limiting similarity one. It is, however, possible to omit/invert procedures by performing extra-checks on input/output data and implementing few pre-procedure steps. I will show how to implement some of them at the end.

# Draw random species from a metaweb

Loading the required libraries and set a random seed

```{r init}
library(assembly)
library(igraph)

set.seed(123)
```

Load the dataset _adirondack_ that comes with _assembly_:

```{r adirondack}
data(adirondack)
```

_adirondack_ is the Adirondack Lakes metaweb as obtained from the GATEWAy database.

`draw_random_species(n, sp.names)` draws random species from a metaweb and requires as input the number of species to draw (*n*) and the species names (*sp.names*). If the metaweb does not have species names, you can assign random ones with `name_metaweb(metaweb)`:

```{r name}
colnames(name_metaweb(matrix(as.numeric(runif(100) > .5), 10, 10)))
```
If names are already present, this will throw an error:

```{r name_error}
tryCatch(name_metaweb(adirondack),
         error = function(e) print(e))
```
When the metaweb has names, define the number of species for the local community:

```{r S}
S <- 50 #species richness
```

And draw a random community, use the function `draw_random_species()`:

```{r random}
sp <- draw_random_species(S, colnames(adirondack))
show_graph(sp, adirondack)
```

## Hidden functions

There are several hidden functions in _assembly_. The reason there are hidden functions is because there is no need to call them directly. Hidden functions can be accessed by prefixing the `assembly:::` (three colon, not two). All hidden functions start with a dot `.`, e.g. `assembly:::.basals()`.

In general, you should not be bothered by hidden functions and should not call them directly, unless you have a good understanding of how they operate. Nevertheless, I summarize them for clarity.

`assembly:::.basals()` get all basal species in the metaweb, and is equivalent to subset the names of the metaweb where `colSums(metaweb) == 0`:

```{r basals}
identical(
  sort(intersect(assembly:::.basals(adirondack), sp)),
  sort(intersect(colnames(adirondack)[colSums(adirondack) == 0], sp))
)
```

`assembly:::.consumers()` and `assembly:::.top()` return the consumers and top consumers of the metaweb, respectively.

`assembly:::.find_isolated()` returns the species that are isolated in the local community:

```{r find_isolated}
assembly:::.find_isolated(sp, adirondack)
```
`assembly:::.find_replacements()` find suitable replacement for the isolated species:

```{r find_replacements}
assembly:::.find_replacements(sp,
                              assembly:::.find_isolated(sp, adirondack),
                              adirondack,
                              keep.n.basal = TRUE)
```

If _keep.n.basal_ is TRUE (default = FALSE), then the original number of basal species will not change.

`assembly:::.move()` performs a move in the limiting similarity procedure (more about this later):

```{r move_fail}
tryCatch(assembly:::.move(sp, adirondack, t = 1),
         error = function(e) print(e))
```

This call to `assembly:::.move()` fails because isolated species are detected in the input. This is a desired property of the function, i.e. it fails when there is an unexpected behavior. All hidden functions have some kind of behavior-check, which is a safety net to assure the code is doing what you asked for.

Finally, `assembly:::.components()` returns the number of connected components in the graph of the local community:

```{r components}
assembly:::.components(sp, adirondack)
```

Usually, a proper food web has only one component, i.e. all species are connected by a path. Having more than one component means that the food web is actually made of several disconnected communities. In the case above, it also means that at least one of this disconnected communities is composed of only one isolated species.

# Resouce filtering

To impose the resource filtering, call the function `resource_filtering()`. This takes as input the species names (*sp.names*), the metaweb (*metaweb*), and an optional argument _keep.n.basal_ to specify weather the original number of basal species should be kept constant (default = `FALSE`). **NOTE this may not be implemented correctly**

Behind the curtain, `resource_filtering()` calls the hidden functions as a way to compress code and make it consistent. That's why you shouldn't bother too much about hidden functions: they're there because they're useful in the development of the package, rather than for your usage. If they're useful for you and you understand how they work, use them.

```{r resource}
sp_resource <- resource_filtering(sp, adirondack, keep.n.basal = TRUE)
show_graph(sp_resource, adirondack)
```

Now the local community is fully connected, i.e. basal species always have a consumer and consumers always have an available resource. It's possible to check this manually calling the hidden functions and working on the adjacency matrix of the local community:

```{r check_connected}
bas <- intersect(sp_resource, assembly:::.basals(adirondack))
cons <- intersect(sp_resource, assembly:::.consumers(adirondack))
all(rowSums(adirondack[bas, cons]) > 0)
all(colSums(adirondack[union(bas, cons), cons]) > 0)
```

Usually you don't need to perform these checks, as I implemented them within  `resource_filtering()`. I also implemented a check for disconnected components, to make sure that the resulting community has no isolated species and only one actual community. 

Bonus: because of these checks, now it is safe to perform a move of the limiting similarity procedure:

```{r move}
assembly:::.move(sp_resource, adirondack, t = 1)
```

# Limiting similarity filtering

The limiting similarity filtering is composed of a series of individuals moves:

  1. A metric ($J$) representing the similarity of interaction is calculated for each species in the community.
  2. One species is removed with probability proportional to their $J$.
  3. The species removed is replaced by another species selected at random from the metaweb.

Each move can then be accepted of discarded based on a probabilistic acceptance criterion, the Metropolis-Hasting algorithm (see below for details). If the move is accepted, the new community will differ from the starting one only in the removed/replaced species. If the move is discarded, the new community is identical to the starting one. Each move takes as input the community of the previous move (or the initial community for the first move) and returns as output the new community (or the original one). 

The limiting similarity procedure is simply a series of moves (accepted or not). The starting community for the limiting similarity procedure is usually a community that has undergone already a resource filtering, imposing thus that trophic competition comes after resource availability. However, it is possible to skip the resource filtering step; the only requirement of the starting community for the limiting similarity filtering is that it does not have isolated species and has only one connected components.

To impose the limiting similariy filtering, call the function `similarity_filtering()`. This function requires the species names as input (*sp.names*), the metaweb (*metaweb*), the argument _t_ (default = 0), which is the temperature of the Metropolis-Hastings algorithm, and _max.iter_ (default = 1,000), which is the maximum number of moves allowed.

```{r limiting}
sp_sim <- similarity_filtering(sp_resource, adirondack, t = 1, max.iter = 10)
show_graph(sp_sim, adirondack)
```

## Metropolis-Hasting algorithm

To accept a move, it must pass a Metropolis-Hasting algorithm. If the similarity of the new community is lower than the similarity of the old one, the move is always accepted. When the new similarity is higher than the old similarity, the move is accepted if:

$$
e^{\left( 1 - \frac{similarity_{new}}{similarity_{old}} \right) \frac{1}{t}} > \mathcal{U}(0, 1)
$$
This means that, even when the new similarity is higher than the old one and the new community has species with increased similarity of interaction, it can still be accepted as a valid move based on a probability density function. The probability of acceptance depends on how much the new similarity is higher than the old one and by the temperature parameter _t_. For increasing _t_, it is more likely to accept a non-favorable move:

```{r metro, fig.height=5, fig.width=6, out.width="80%"}
temp <- 10 ^ seq(-2, 1, by = .1)
ratio <- 10 ^ seq(-1, 2, by = .1)

move <- rep(NA, length(temp) * length(ratio))
i <- 1
for (t in temp){
  for (x in ratio) {
    move[i] <- metropolis.hastings(1, x, t)
    i <- i + 1
  }
}

d <- data.frame(Temp = rep(temp, each = length(ratio)),
                Ratio = rep(ratio, length(temp)),
                Move = as.numeric(move))
cols <- colorRampPalette(c("steelblue", "tomato"))
cols <- cols(length(unique(d$Temp)))
pal <- adjustcolor(cols, alpha.f = .2)
pal <- pal[sapply(d$Temp, \(x) which(unique(d$Temp) == x))]
plot(log10(d$Ratio), jitter(d$Move, factor = .2), 
     xlab = "Similiarity ratio (new / old)",
     ylab = "Accepted move",
     col = pal, pch = 20, frame = FALSE)
for (t in unique(d$Temp)) {
  x <- log10(d$Ratio[d$Temp == t])
  y <- d$Move[d$Temp == t]
  fit <- loess(y ~ x)
  lines(fit$x, fit$fitted, col = cols[which(unique(d$Temp) == t)])
}
legend(-1, .7, legend = seq(-2, 1, by = .5), 
       fill = colorRampPalette(c("steelblue", "tomato"))(7),
       title = "log10(t)")
```

The reason to include this algorithm is to avoid a purely deterministic procedure and include some stochasticity in the process. However, if this is unwanted, it can be removed (and the process made purely deterministic), by specifying a very high temperature parameter _t_:

```{r tweak_t}
table(sapply(seq_len(1000), \(x) metropolis.hastings(1, 1e3, t = 1e9)))
```
